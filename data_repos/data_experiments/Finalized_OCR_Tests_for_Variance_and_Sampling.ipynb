{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import os\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.models import ldamodel, doc2vec, LsiModel \n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import string\n",
    "import csv\n",
    "import math\n",
    "import statistics\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "# nltk.download('stopwords')\n",
    "from collections import OrderedDict, Counter, namedtuple\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.readwrite import json_graph\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "from bokeh.layouts import row, column\n",
    "import random\n",
    "import codecs, difflib, Levenshtein, distance\n",
    "import rpy2\n",
    "from datasketch import MinHash\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# nlp = spacy.load('en_core_web_lg')\n",
    "%load_ext rpy2.ipython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R \n",
    "install.packages(\"textreuse\", repos='http://cran.us.r-project.org', quiet=TRUE)\n",
    "install.packages(\"readr\", repos='http://cran.us.r-project.org', quiet=TRUE)\n",
    "library(\"textreuse\")\n",
    "library(\"readr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First Test: Egyptian Gazette 1947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_unordered = pd.read_csv('ocr_test_newspaper_egyptian_gazette_one_page_unordered.csv')\n",
    "eg_ordered = pd.read_csv('ocr_test_newspaper_egyptian_gazette_one_page_ordered.csv')\n",
    "ocr_values = [eg_unordered['base_file_name'].iloc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Arab Scribe January 5 1964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eg_unordered = pd.read_csv('ocr_test_magazine_arab_scribe_unordered.csv')\n",
    "eg_ordered = pd.read_csv('ocr_test_magazine_arab_scribe_ordered.csv')\n",
    "\n",
    "eg_ordered['contains_image'].fillna(value=False, inplace=True)\n",
    "\n",
    "for index, row in eg_ordered.iterrows():\n",
    "    if math.isnan(row['page_number']):\n",
    "        pgn = row['base_file_name'].split('imagefile')[0][-3:]\n",
    "        pgn = pgn.split('_')[0]\n",
    "        eg_ordered.loc[index, 'page_number'] = int(pgn)\n",
    "\n",
    "groupby_df = eg_ordered.groupby('page_number')['google_vision_text'].apply(' '.join).reset_index()\n",
    "eg_ordered = eg_ordered.drop_duplicates(subset=['page_number'], keep='first')\n",
    "eg_ordered = eg_ordered.drop(columns='google_vision_text')\n",
    "final_df = pd.merge(eg_ordered, groupby_df, on='page_number', how='outer')\n",
    "eg_ordered = final_df.drop(columns='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words w/ or w/o punctuation and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenize(text):\n",
    "    if not text:\n",
    "#       print('The text to be tokenized is a None type. Defaulting to blank string.')\n",
    "        text = ''\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def process_text(df, punc):\n",
    "\n",
    "    final_doc = []\n",
    "    for index, row in df.iterrows():\n",
    "        page = []\n",
    "        raw_text = row['google_vision_text']\n",
    "        tokens = custom_tokenize(raw_text)\n",
    "        for t in tokens:\n",
    "            if punc:\n",
    "                if t.lower() in string.punctuation:\n",
    "                    continue\n",
    "                elif t.lower() in stopwords.words('english'):\n",
    "                    continue\n",
    "                else:\n",
    "                    final_doc.append(t.lower())\n",
    "                    page.append(t.lower())\n",
    "            else: \n",
    "                final_doc.append(t.lower())\n",
    "    text = ' '.join(final_doc)\n",
    "    return final_doc, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_list, order_text = process_text(eg_ordered, True)\n",
    "unorder_list, unorder_text = process_text(eg_unordered, True)\n",
    "all_documents = [order_text, unorder_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_order = eg_ordered.sample(frac=1).reset_index(drop=True)\n",
    "random_unorder = eg_unordered.sample(frac=1).reset_index(drop=True)\n",
    "rorder_list, rorder_text = process_text(random_order, True)\n",
    "runorder_list, runorder_text = process_text(random_unorder, True)\n",
    "random_all_documents = [rorder_text, runorder_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_full_issue(all_documents, ocr_values, full_issues_ocr):\n",
    "    # Count n grams frequencies and calculate cosine similarity between two docs. \n",
    "    counts = CountVectorizer(ngram_range=(1,5))\n",
    "    counts_matrix = counts.fit_transform(all_documents)\n",
    "    cos = cosine_similarity(counts_matrix[0:1], counts_matrix)\n",
    "    print('Count Vectorizer', cos[0][1])\n",
    "    ocr_values.append(cos[0][1])\n",
    "    \n",
    "    # Calculate tf-idf cosine similarity (nltk or spacy text the same)\n",
    "    tokenize = lambda doc: doc.lower().split(\" \")\n",
    "    tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize, ngram_range=(1,1))\n",
    "    tfidf_matrix = tfidf.fit_transform(all_documents)\n",
    "\n",
    "    cos = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
    "    print('TF-IDF Vectorizer', cos)\n",
    "    ocr_values.append(cos[0][1])\n",
    "    \n",
    "    # Calculate similarity using GLOVE and SPACY\n",
    "    order_doc = nlp(order_text)\n",
    "    unorder_doc = nlp(unorder_text)\n",
    "    sim_doc = order_doc.similarity(unorder_doc)\n",
    "    print('Spacy GLOVE', sim_doc)\n",
    "    #https://stats.stackexchange.com/questions/304217/how-is-the-similarity-method-in-spacy-computed\n",
    "    ocr_values.append(sim_doc)\n",
    "    \n",
    "    # Calculate jaccard ratio. Takes list of tokens\n",
    "    jac = 1 - distance.jaccard(order_list, unorder_list)\n",
    "    print('Jaccard', jac)\n",
    "    ocr_values.append(jac)\n",
    "    \n",
    "    # use gensim's similarity matrix and lsi to calculate cosine\n",
    "    all_tokens = [order_list, unorder_list]\n",
    "    dictionary = Dictionary(all_tokens)\n",
    "    corpus = [dictionary.doc2bow(text) for text in all_tokens]\n",
    "    lsi = LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "    sim = MatrixSimilarity(lsi[corpus])\n",
    "    lsi_cos = [ t[1][1] for t in list(enumerate(sim))]\n",
    "    lsi_cos = lsi_cos[0]\n",
    "    print('LSI', lsi_cos)\n",
    "    ocr_values.append(lsi_cos)\n",
    "    #https://radimrehurek.com/gensim/tut3.html\n",
    "    \n",
    "    if os.path.isfile(full_issues_ocr):\n",
    "        final_metrics = pd.read_csv(full_issues_ocr)\n",
    "        ocr_values.append(datetime.date.today())\n",
    "        final_metrics.loc[len(final_metrics.index)] = ocr_values\n",
    "        final_metrics.to_csv(full_issues_ocr, index=False)\n",
    "    else:\n",
    "        cols = ['base_file_name', 'num_pages', 'countsvec_cos', 'tfidfvec_cos', 'spacy_sim', 'jaccard_sim', 'lsi_cos', 'date_run']\n",
    "        ocr_values.append(datetime.date.today())\n",
    "        final_df = pd.DataFrame([ocr_values], columns=cols)\n",
    "        final_df.to_csv(full_issues_ocr, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer 0.959538261301\n",
      "TF-IDF Vectorizer [[ 1.          0.93357973]]\n",
      "Spacy GLOVE 0.999952728742\n",
      "Jaccard 0.8636978579481398\n",
      "LSI 0.997655\n"
     ]
    }
   ],
   "source": [
    "ocr_values = [eg_unordered['base_file_name'].iloc[0], len(eg_unordered.index)]\n",
    "process_full_issue(all_documents, ocr_values, 'ocr_accuracy_full_issue_arab_scribe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_pages(order, unorder):\n",
    "    %%R -i order\n",
    "    %%R -i unorder\n",
    "#     order <- read_file(\"order_doc.txt\")\n",
    "#     unorder <- read_file(\"unorder_doc.txt\")\n",
    "    %%R perfect = align_local(order, order)\n",
    "    %%R actual = align_local(order, unorder)\n",
    "    %%R smw <- actual$score / perfect$score\n",
    "    %%R smw\n",
    "    %%R -o smw\n",
    "    print(smw)\n",
    "    return smw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_page(all_documents, order_text, unorder_text, order_list, unorder_list, ocr_values, page_ocr):\n",
    "    # Count n grams frequencies and calculate cosine similarity between two docs. \n",
    "    counts = CountVectorizer(ngram_range=(1,5))\n",
    "    counts_matrix = counts.fit_transform(all_documents)\n",
    "    cos = cosine_similarity(counts_matrix[0:1], counts_matrix)\n",
    "    print('Count Vectorizer', cos[0][1])\n",
    "    ocr_values.append(cos[0][1])\n",
    "    \n",
    "    # Calculate tf-idf cosine similarity (nltk or spacy text the same)\n",
    "    tokenize = lambda doc: doc.lower().split(\" \")\n",
    "    tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize, ngram_range=(1,1))\n",
    "    tfidf_matrix = tfidf.fit_transform(all_documents)\n",
    "\n",
    "    cos = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
    "    print('TF-IDF Vectorizer', cos)\n",
    "    ocr_values.append(cos[0][1])\n",
    "    \n",
    "#     # Calculate similarity using GLOVE and SPACY\n",
    "#     order_doc = nlp(order_text)\n",
    "#     unorder_doc = nlp(unorder_text)\n",
    "#     sim_doc = order_doc.similarity(unorder_doc)\n",
    "#     print('Spacy GLOVE', sim_doc)\n",
    "#     #https://stats.stackexchange.com/questions/304217/how-is-the-similarity-method-in-spacy-computed\n",
    "#     ocr_values.append(sim_doc)\n",
    "    \n",
    "    # Calculate jaccard ratio. Takes list of tokens\n",
    "    jac = 1 - distance.jaccard(order_list, unorder_list)\n",
    "    print('Jaccard', jac)\n",
    "    ocr_values.append(jac)\n",
    "    \n",
    "    # use gensim's similarity matrix and lsi to calculate cosine\n",
    "    all_tokens = [order_list, unorder_list]\n",
    "    dictionary = Dictionary(all_tokens)\n",
    "    corpus = [dictionary.doc2bow(text) for text in all_tokens]\n",
    "    lsi = LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "    sim = MatrixSimilarity(lsi[corpus])\n",
    "    lsi_cos = [ t[1][1] for t in list(enumerate(sim))]\n",
    "    lsi_cos = lsi_cos[0]\n",
    "    print('LSI', lsi_cos)\n",
    "    ocr_values.append(lsi_cos)\n",
    "    #https://radimrehurek.com/gensim/tut3.html\n",
    "    \n",
    "#     align = align_pages(order_text, unorder_text)\n",
    "#     print('smw', align)\n",
    "#     ocr_values.append(align)\n",
    "    \n",
    "#     if os.path.isfile(page_ocr):\n",
    "#         final_metrics = pd.read_csv(page_ocr)\n",
    "#         ocr_values.append(len(order_text))\n",
    "#         ocr_values.append(len(unorder_text))\n",
    "#         ocr_values.append(datetime.date.today())\n",
    "#         final_metrics.loc[len(final_metrics.index)] = ocr_values\n",
    "#         print(final_metrics)\n",
    "#         final_metrics.to_csv(page_ocr, index=False)\n",
    "#     else:\n",
    "#         ocr_values.append(len(order_text))\n",
    "#         ocr_values.append(len(unorder_text))\n",
    "#         ocr_values.append(datetime.date.today())\n",
    "#         cols = ['base_file_name', 'page_number', 'countsvec_cos', 'tfidfvec_cos', 'spacy_sim', 'jaccard_sim', 'lsi_cos','smw_align', 'len_order', 'len_unorder', 'date_run']\n",
    "#         final_df = pd.DataFrame([ocr_values], columns=cols)\n",
    "#         final_df.to_csv(page_ocr, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.82814880195\n",
      "TF-IDF Vectorizer [[ 1.          0.89692841]]\n",
      "Jaccard 0.8888888888888888\n",
      "LSI 0.960863\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.577078469596\n",
      "TF-IDF Vectorizer [[ 1.          0.90497357]]\n",
      "Jaccard 0.902127659574468\n",
      "LSI 0.97168\n",
      "Count Vectorizer 0.667523014697\n",
      "TF-IDF Vectorizer [[ 1.          0.98945976]]\n",
      "Jaccard 0.9911504424778761\n",
      "LSI 0.995149\n",
      "Count Vectorizer 0.880603989861\n",
      "TF-IDF Vectorizer [[ 1.         0.8319078]]\n",
      "Jaccard 0.8493975903614458\n",
      "LSI 0.938007\n",
      "Count Vectorizer 0.502186625725\n",
      "TF-IDF Vectorizer [[ 1.          0.95695887]]\n",
      "Jaccard 0.9532967032967034\n",
      "LSI 0.985997\n",
      "Count Vectorizer 0.719260322127\n",
      "TF-IDF Vectorizer [[ 1.          0.76826979]]\n",
      "Jaccard 0.7668393782383419\n",
      "LSI 0.930278\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.474552860472\n",
      "TF-IDF Vectorizer [[ 1.          0.83136576]]\n",
      "Jaccard 0.8299319727891157\n",
      "LSI 0.931771\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.851724372712\n",
      "TF-IDF Vectorizer [[ 1.          0.91240776]]\n",
      "Jaccard 0.9180327868852459\n",
      "LSI 0.973958\n",
      "Count Vectorizer 0.816490186146\n",
      "TF-IDF Vectorizer [[ 1.          0.85566261]]\n",
      "Jaccard 0.8458646616541353\n",
      "LSI 0.956706\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.946835443038\n",
      "TF-IDF Vectorizer [[ 1.         0.9858546]]\n",
      "Jaccard 0.9866071428571429\n",
      "LSI 0.997003\n",
      "Count Vectorizer 0.973760932945\n",
      "TF-IDF Vectorizer [[ 1.          0.99135887]]\n",
      "Jaccard 0.9916317991631799\n",
      "LSI 0.997947\n",
      "Count Vectorizer 0.806949090087\n",
      "TF-IDF Vectorizer [[ 1.          0.81413656]]\n",
      "Jaccard 0.7958115183246073\n",
      "LSI 0.956607\n",
      "Count Vectorizer 0.913347192754\n",
      "TF-IDF Vectorizer [[ 1.          0.92842249]]\n",
      "Jaccard 0.926530612244898\n",
      "LSI 0.980468\n",
      "Count Vectorizer 0.990655068179\n",
      "TF-IDF Vectorizer [[ 1.          0.99130361]]\n",
      "Jaccard 0.991701244813278\n",
      "LSI 0.997076\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.87419598227\n",
      "TF-IDF Vectorizer [[ 1.          0.90208785]]\n",
      "Jaccard 0.8991825613079019\n",
      "LSI 0.974336\n",
      "Count Vectorizer 0.633563166159\n",
      "TF-IDF Vectorizer [[ 1.          0.89559839]]\n",
      "Jaccard 0.8708609271523179\n",
      "LSI 0.984509\n",
      "Count Vectorizer 0.634213612304\n",
      "TF-IDF Vectorizer [[ 1.         0.8548374]]\n",
      "Jaccard 0.8038147138964578\n",
      "LSI 0.977318\n",
      "Count Vectorizer 0.72807190848\n",
      "TF-IDF Vectorizer [[ 1.          0.82936111]]\n",
      "Jaccard 0.8043478260869565\n",
      "LSI 0.980841\n",
      "Count Vectorizer 0.659085356421\n",
      "TF-IDF Vectorizer [[ 1.          0.89578129]]\n",
      "Jaccard 0.875\n",
      "LSI 0.982964\n",
      "Count Vectorizer 0.618982550732\n",
      "TF-IDF Vectorizer [[ 1.          0.91959905]]\n",
      "Jaccard 0.9142091152815014\n",
      "LSI 0.984869\n",
      "Count Vectorizer 0.562029676817\n",
      "TF-IDF Vectorizer [[ 1.          0.90059139]]\n",
      "Jaccard 0.8841607565011821\n",
      "LSI 0.974194\n",
      "Count Vectorizer 0.575053092114\n",
      "TF-IDF Vectorizer [[ 1.          0.86934994]]\n",
      "Jaccard 0.8487584650112867\n",
      "LSI 0.969162\n",
      "Count Vectorizer 0.528474751149\n",
      "TF-IDF Vectorizer [[ 1.          0.86797561]]\n",
      "Jaccard 0.8542274052478134\n",
      "LSI 0.966846\n",
      "Count Vectorizer 0.98749747979\n",
      "TF-IDF Vectorizer [[ 1.          0.98731255]]\n",
      "Jaccard 0.9873949579831933\n",
      "LSI 0.994804\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.988042161277\n",
      "TF-IDF Vectorizer [[ 1.          0.97543485]]\n",
      "Jaccard 0.9720670391061452\n",
      "LSI 0.995496\n",
      "Count Vectorizer 0.904033847402\n",
      "TF-IDF Vectorizer [[ 1.          0.94973967]]\n",
      "Jaccard 0.9539473684210527\n",
      "LSI 0.979815\n",
      "Count Vectorizer 0.92819716255\n",
      "TF-IDF Vectorizer [[ 1.          0.92033966]]\n",
      "Jaccard 0.9136212624584718\n",
      "LSI 0.973096\n",
      "Count Vectorizer 0.896393171959\n",
      "TF-IDF Vectorizer [[ 1.          0.90671085]]\n",
      "Jaccard 0.8984126984126984\n",
      "LSI 0.984964\n",
      "Count Vectorizer 0.928668766588\n",
      "TF-IDF Vectorizer [[ 1.          0.93761594]]\n",
      "Jaccard 0.9367816091954023\n",
      "LSI 0.983998\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.966603252323\n",
      "TF-IDF Vectorizer [[ 1.          0.99165675]]\n",
      "Jaccard 0.9908814589665653\n",
      "LSI 0.997784\n",
      "Count Vectorizer 0.932386548592\n",
      "TF-IDF Vectorizer [[ 1.          0.94337992]]\n",
      "Jaccard 0.9314641744548287\n",
      "LSI 0.992686\n",
      "Count Vectorizer 0.963369685851\n",
      "TF-IDF Vectorizer [[ 1.          0.98207779]]\n",
      "Jaccard 0.976897689768977\n",
      "LSI 0.969442\n",
      "Count Vectorizer 0.990709092358\n",
      "TF-IDF Vectorizer [[ 1.         0.9669979]]\n",
      "Jaccard 0.9689119170984456\n",
      "LSI 0.991279\n",
      "Count Vectorizer 0.69114254955\n",
      "TF-IDF Vectorizer [[ 1.          0.92590264]]\n",
      "Jaccard 0.9074889867841409\n",
      "LSI 0.980993\n",
      "Count Vectorizer 0.662669934651\n",
      "TF-IDF Vectorizer [[ 1.          0.89402393]]\n",
      "Jaccard 0.8832684824902723\n",
      "LSI 0.967407\n",
      "Count Vectorizer 0.963809715503\n",
      "TF-IDF Vectorizer [[ 1.          0.94937558]]\n",
      "Jaccard 0.9465020576131687\n",
      "LSI 0.981867\n",
      "Count Vectorizer 0.980273831534\n",
      "TF-IDF Vectorizer [[ 1.          0.96113456]]\n",
      "Jaccard 0.9509594882729211\n",
      "LSI 0.992636\n",
      "Count Vectorizer 0.810202347697\n",
      "TF-IDF Vectorizer [[ 1.          0.86816169]]\n",
      "Jaccard 0.8461538461538461\n",
      "LSI 0.955569\n",
      "Count Vectorizer 0.961273176471\n",
      "TF-IDF Vectorizer [[ 1.          0.93044323]]\n",
      "Jaccard 0.9299610894941635\n",
      "LSI 0.98071\n",
      "Count Vectorizer 0.858100334506\n",
      "TF-IDF Vectorizer [[ 1.          0.84667722]]\n",
      "Jaccard 0.8217213114754098\n",
      "LSI 0.954705\n",
      "Count Vectorizer 0.6785670429\n",
      "TF-IDF Vectorizer [[ 1.          0.93496033]]\n",
      "Jaccard 0.9319148936170213\n",
      "LSI 0.975634\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.693551369325\n",
      "TF-IDF Vectorizer [[ 1.          0.75613535]]\n",
      "Jaccard 0.7571428571428571\n",
      "LSI 0.915119\n",
      "Count Vectorizer 0.476796495833\n",
      "TF-IDF Vectorizer [[ 1.          0.95962499]]\n",
      "Jaccard 0.9585365853658536\n",
      "LSI 0.989691\n",
      "Count Vectorizer 0.459274133044\n",
      "TF-IDF Vectorizer [[ 1.         0.8034958]]\n",
      "Jaccard 0.808695652173913\n",
      "LSI 0.935061\n",
      "Count Vectorizer 0.983587338804\n",
      "TF-IDF Vectorizer [[ 1.          0.93242533]]\n",
      "Jaccard 0.9389312977099237\n",
      "LSI 0.979217\n",
      "Count Vectorizer 0.658715241053\n",
      "TF-IDF Vectorizer [[ 1.          0.82650407]]\n",
      "Jaccard 0.83125\n",
      "LSI 0.952055\n",
      "Count Vectorizer 0.591725660526\n",
      "TF-IDF Vectorizer [[ 1.          0.96466136]]\n",
      "Jaccard 0.9661654135338346\n",
      "LSI 0.9869\n",
      "Count Vectorizer 0.849881859798\n",
      "TF-IDF Vectorizer [[ 1.          0.86938029]]\n",
      "Jaccard 0.8571428571428571\n",
      "LSI 0.960167\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.718485423314\n",
      "TF-IDF Vectorizer [[ 1.          0.78574854]]\n",
      "Jaccard 0.822463768115942\n",
      "LSI 0.90737\n",
      "Count Vectorizer 0.908048376403\n",
      "TF-IDF Vectorizer [[ 1.         0.9576097]]\n",
      "Jaccard 0.9655172413793104\n",
      "LSI 0.971544\n",
      "Count Vectorizer 0.725624047826\n",
      "TF-IDF Vectorizer [[ 1.         0.7841877]]\n",
      "Jaccard 0.7594339622641509\n",
      "LSI 0.940026\n",
      "Count Vectorizer 0.937202338053\n",
      "TF-IDF Vectorizer [[ 1.          0.95918673]]\n",
      "Jaccard 0.9681818181818181\n",
      "LSI 0.98756\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.35646407744\n",
      "TF-IDF Vectorizer [[ 1.          0.64248671]]\n",
      "Jaccard 0.6577946768060836\n",
      "LSI 0.827565\n",
      "Count Vectorizer 0.451916258538\n",
      "TF-IDF Vectorizer [[ 1.         0.9582515]]\n",
      "Jaccard 0.9597156398104265\n",
      "LSI 0.988241\n",
      "Count Vectorizer 0.411389303938\n",
      "TF-IDF Vectorizer [[ 1.          0.80907121]]\n",
      "Jaccard 0.823045267489712\n",
      "LSI 0.923703\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.446117634146\n",
      "TF-IDF Vectorizer [[ 1.          0.77091201]]\n",
      "Jaccard 0.8025210084033614\n",
      "LSI 0.920801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer 0.877887273731\n",
      "TF-IDF Vectorizer [[ 1.          0.90131917]]\n",
      "Jaccard 0.92\n",
      "LSI 0.964979\n",
      "Count Vectorizer 0.565495039179\n",
      "TF-IDF Vectorizer [[ 1.          0.81886335]]\n",
      "Jaccard 0.8441558441558441\n",
      "LSI 0.935804\n",
      "Count Vectorizer 0.946328221974\n",
      "TF-IDF Vectorizer [[ 1.          0.93843714]]\n",
      "Jaccard 0.9318181818181818\n",
      "LSI 0.987714\n",
      "Count Vectorizer 0.607973599046\n",
      "TF-IDF Vectorizer [[ 1.          0.78125839]]\n",
      "Jaccard 0.7939110070257611\n",
      "LSI 0.923838\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 1.0\n",
      "TF-IDF Vectorizer [[ 1.  1.]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(eg_unordered.index)):\n",
    "    ocr_values = [eg_unordered['base_file_name'].iloc[i], i]\n",
    "    order_list, order_text = process_text(eg_ordered.iloc[[i]], True)\n",
    "    unorder_list, unorder_text = process_text(eg_unordered.iloc[[i]], True)\n",
    "    all_documents = [order_text, unorder_text]\n",
    "    process_page(all_documents, order_text, unorder_text, order_list, unorder_list, ocr_values, 'ocr_accuracy_page_level_arab_scribe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n",
      "Count Vectorizer 0.94301968224\n",
      "TF-IDF Vectorizer [[ 1.         0.9322488]]\n",
      "Jaccard 1.0\n",
      "LSI 1.0\n"
     ]
    }
   ],
   "source": [
    "random_order = eg_ordered.sample(frac=1).reset_index(drop=True)\n",
    "random_unorder = eg_unordered.sample(frac=1).reset_index(drop=True)\n",
    "rorder_list, rorder_text = process_text(random_order, True)\n",
    "runorder_list, runorder_text = process_text(random_unorder, True)\n",
    "random_all_documents = [rorder_text, runorder_text]\n",
    "for i in range(0, len(random_order.index)):\n",
    "    ocr_values = [random_order['base_file_name'].iloc[i], i]\n",
    "    rorder_list, rorder_text = process_text(random_order.iloc[[i]], True)\n",
    "    order_list, order_text = process_text(eg_ordered.iloc[[i]], True)\n",
    "#     runorder_list, runorder_text = process_text(random_unorder.iloc[[i]], True)\n",
    "    random_all_documents = [rorder_text, order_text]\n",
    "    process_page(all_documents, rorder_text, order_text, rorder_list, rorder_list, ocr_values, 'ocr_accuracy_page_level_arab_scribe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count n grams frequencies and calculate cosine similarity between two docs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.959538261301\n"
     ]
    }
   ],
   "source": [
    "counts = CountVectorizer(ngram_range=(1,5))\n",
    "counts_matrix = counts.fit_transform(all_documents)\n",
    "cos = cosine_similarity(counts_matrix[0:1], counts_matrix)\n",
    "print(cos[0][1])\n",
    "ocr_values.append(cos[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tf-idf cosine similarity (nltk or spacy text the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.93357973]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize, ngram_range=(1,1))\n",
    "tfidf_matrix = tfidf.fit_transform(all_documents)\n",
    "\n",
    "cos = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
    "print(cos)\n",
    "ocr_values.append(cos[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity using GLOVE and SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999952728742\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "order_doc = nlp(order_text)\n",
    "unorder_doc = nlp(unorder_text)\n",
    "sim_doc = order_doc.similarity(unorder_doc)\n",
    "print(sim_doc)\n",
    "#https://stats.stackexchange.com/questions/304217/how-is-the-similarity-method-in-spacy-computed\n",
    "ocr_values.append(sim_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205680"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write spacy texts for R\n",
    "f = open('order_doc.txt', 'wt', encoding='utf-8')\n",
    "f.write(order_text)\n",
    "f = open('unorder_doc.txt', 'wt', encoding='utf-8')\n",
    "f.write(unorder_text)\n",
    "#Create tokens from spacy tokens\n",
    "# order_doc_tokens = []\n",
    "# for t in order_doc:\n",
    "#     order_doc_tokens.append(t.text)\n",
    "# unorder_doc_tokens = []\n",
    "# for t in jane_doc:\n",
    "#     unorder_doc_tokens.append(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with completely random text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(unorder_text))\n",
    "# with open('jane_austen.txt', 'r') as myfile:\n",
    "#   jane = myfile.read()\n",
    "# jane = jane[:17457]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate jaccard ratio. Takes list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersection : new set with elements common to s and t\n",
    "# union : new set with elements from both s and t\n",
    "# difference: new set with elements in s but not in t\n",
    "# symmetric difference: new set with elements in either s or t but not both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8636978579481398\n"
     ]
    }
   ],
   "source": [
    "jac = 1 - distance.jaccard(order_list, unorder_list)\n",
    "print(jac)\n",
    "# m1, m2 = MinHash(num_perm=256), MinHash(num_perm=256)\n",
    "# for d in order_list:\n",
    "#     m1.update(d.encode('utf8'))\n",
    "# for d in unorder_list:\n",
    "#     m2.update(d.encode('utf8'))\n",
    "# print(\"Estimated Jaccard for data1 and data2 is\", m1.jaccard(m2))\n",
    "ocr_values.append(jac)\n",
    "# ocr_values.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gensim's similarity matrix and lsi to calculate cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.997655\n"
     ]
    }
   ],
   "source": [
    "all_tokens = [order_list, unorder_list]\n",
    "dictionary = Dictionary(all_tokens)\n",
    "corpus = [dictionary.doc2bow(text) for text in all_tokens]\n",
    "lsi = LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "sim = MatrixSimilarity(lsi[corpus])\n",
    "lsi_cos = [ t[1][1] for t in list(enumerate(sim))]\n",
    "lsi_cos = lsi_cos[0]\n",
    "print(lsi_cos)\n",
    "ocr_values.append(lsi_cos)\n",
    "#https://radimrehurek.com/gensim/tut3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use textreuse align local for Smith Waterman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R \n",
    "install.packages(\"textreuse\", repos='http://cran.us.r-project.org', quiet=TRUE)\n",
    "install.packages(\"readr\", repos='http://cran.us.r-project.org', quiet=TRUE)\n",
    "library(\"textreuse\")\n",
    "library(\"readr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -o smw\n",
    "order <- read_file(\"order_doc.txt\")\n",
    "unorder <- read_file(\"unorder_doc.txt\")\n",
    "perfect = align_local(order, order)\n",
    "actual = align_local(order, unorder)\n",
    "actual\n",
    "smw <- actual$score / perfect$score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_values.append(smw[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_file_name</th>\n",
       "      <th>countsvec_cos</th>\n",
       "      <th>tfidfvec_cos</th>\n",
       "      <th>spacy_sim</th>\n",
       "      <th>jaccard_sim</th>\n",
       "      <th>smw_align</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_lucida_app/media/Egyptian_Gazette_1947_J...</td>\n",
       "      <td>0.524825</td>\n",
       "      <td>0.58027</td>\n",
       "      <td>0.999314</td>\n",
       "      <td>0.522356</td>\n",
       "      <td>0.019961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      base_file_name  countsvec_cos  \\\n",
       "0  image_lucida_app/media/Egyptian_Gazette_1947_J...       0.524825   \n",
       "\n",
       "   tfidfvec_cos  spacy_sim  jaccard_sim  smw_align  \n",
       "0       0.58027   0.999314     0.522356   0.019961  "
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['base_file_name', 'countsvec_cos', 'tfidfvec_cos', 'spacy_sim', 'jaccard_sim', 'lsi_cos', 'smw_align']\n",
    "final_df = pd.DataFrame([ocr_values], columns=cols)\n",
    "final_df\n",
    "# final_df.to_csv('ocr_quality_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An important class of problems that Jaccard similarity addresses well is that of finding textually similar documents in a large corpus such as the Web or a collection of news articles. We should understand that the aspect of similarity we are looking at here is character-level similarity, not “similar meaning,” which requires us to examine the words in the documents and their uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram(df):\n",
    "\n",
    "    num_words = []\n",
    "    for index, row in df.iterrows():\n",
    "        raw_text = row['google_vision_text']\n",
    "        sents = nltk.sent_tokenize(raw_text)\n",
    "        for s in sents:\n",
    "            tokens = nltk.word_tokenize(s)\n",
    "            tokens = [t for t in tokens if t not in stopwords.words('english')]\n",
    "            num_words.append(len(tokens))\n",
    "    print(num_words)\n",
    "    med = statistics.median(num_words)\n",
    "    avg = statistics.mean(num_words)\n",
    "    pstd = statistics.pstdev(num_words)\n",
    "    sstd = statistics.stdev(num_words)\n",
    "    return med, avg, pstd, sstd\n",
    "\n",
    "val1, val2, val3, val4 = get_ngram(eg_ordered)\n",
    "print(val1, val2, val3, val4)\n",
    "\n",
    "def count_ngrams(words, ngram):\n",
    "    counts_n = ngrams(words,ngram)\n",
    "    collects_n = Counter(counts_n)\n",
    "    print(collects_n)\n",
    "    \n",
    "def smith_waterman(a: str, b: str, alignment_score: float = 1, gap_cost: float = 1) -> float:\n",
    "  \"\"\"\n",
    "  Compute the Smith-Waterman alignment score for two strings.\n",
    "  See https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm#Algorithm\n",
    "  This implementation has a fixed gap cost (i.e. extending a gap is considered\n",
    "  free). In the terminology of the Wikipedia description, W_k = {c, c, c, ...}.\n",
    "  This implementation also has a fixed alignment score, awarded if the relevant\n",
    "  characters are equal.\n",
    "  Kinda slow, especially for large (50+ char) inputs.\n",
    "  https://gist.github.com/nornagon/6326a643fc30339ece3021013ed9b48c\n",
    "  \"\"\"\n",
    "  # H holds the alignment score at each point, computed incrementally\n",
    "  H = np.zeros((len(a) + 1, len(b) + 1))\n",
    "  for i in range(1, len(a) + 1):\n",
    "    for j in range(1, len(b) + 1):\n",
    "      # The score for substituting the letter a[i-1] for b[j-1]. Generally low\n",
    "      # for mismatch, high for match.\n",
    "      match = H[i-1,j-1] + (alignment_score if a[i-1] == b[j-1] else 0)\n",
    "      # The scores for for introducing extra letters in one of the strings (or\n",
    "      # by symmetry, deleting them from the other).\n",
    "      delete = H[1:i,j].max() - gap_cost if i > 1 else 0\n",
    "      insert = H[i,1:j].max() - gap_cost if j > 1 else 0\n",
    "      H[i,j] = max(match, delete, insert, 0)\n",
    "  # The highest score is the best local alignment.\n",
    "  # For our purposes, we don't actually care _what_ the alignment was, just how\n",
    "  # aligned the two strings were.\n",
    "  return H.max()\n",
    "\n",
    "def smith_waterman_distance(seq1, seq2, match=3, mismatch=-1, insertion=-1, deletion=-1, normalize=1):\n",
    "    '''simple and general smith waterman distance for NLP feature extraction'''\n",
    "    # switch sequences, so that seq1 is the longer sequence to search for seq2\n",
    "    if len(seq2) > len(seq1): seq1, seq2 = seq2, seq1\n",
    "    # create the distance matrix\n",
    "    mat = np.zeros((len(seq2) + 1, len(seq1) + 1))\n",
    "    # iterate over the matrix column wise\n",
    "    for i in range(1, mat.shape[0]):\n",
    "        # iterate over the matrix row wise\n",
    "        for j in range(1, mat.shape[1]):\n",
    "            # set the current matrix element with the maximum of 4 different cases\n",
    "            mat[i, j] = max(\n",
    "                # negative values are not allowed\n",
    "                0,\n",
    "                # if previous character matches increase the score by match, else decrease it by mismatch\n",
    "                mat[i - 1, j - 1] + (match if seq1[j - 1] == seq2[i - 1] else mismatch),\n",
    "                # one character is missing in seq2, so decrease the score by deletion\n",
    "                mat[i - 1, j] + deletion,\n",
    "                # one additional character is in seq2, so decrease the scare by insertion\n",
    "                mat[i, j - 1] + insertion\n",
    "            )\n",
    "    # the maximum of mat is now the score, which is returned raw or normalized (with a range of 0-1)\n",
    "    return np.max(mat) / (len(seq2) * match) if normalize else np.max(mat)\n",
    "\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "def term_frequency(term, tokenized_document):\n",
    "    return tokenized_document.count(term)\n",
    "\n",
    "def sublinear_term_frequency(term, tokenized_document):\n",
    "    count = tokenized_document.count(term)\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    return 1 + math.log(count)\n",
    "\n",
    "def augmented_term_frequency(term, tokenized_document):\n",
    "    max_count = max([term_frequency(t, tokenized_document) for t in tokenized_document])\n",
    "    return (0.5 + ((0.5 * term_frequency(term, tokenized_document))/max_count))\n",
    "\n",
    "def inverse_document_frequencies(tokenized_documents):\n",
    "    idf_values = {}\n",
    "    all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "    for tkn in all_tokens_set:\n",
    "        contains_token = map(lambda doc: tkn in doc, tokenized_documents)\n",
    "        idf_values[tkn] = 1 + math.log(len(tokenized_documents)/(sum(contains_token)))\n",
    "    return idf_values\n",
    "\n",
    "def tfidf(documents):\n",
    "    tokenized_documents = [tokenize(d) for d in documents]\n",
    "    idf = inverse_document_frequencies(tokenized_documents)\n",
    "    tfidf_documents = []\n",
    "    for document in tokenized_documents:\n",
    "        doc_tfidf = []\n",
    "        for term in idf.keys():\n",
    "            tf = sublinear_term_frequency(term, document)\n",
    "            doc_tfidf.append(tf * idf[term])\n",
    "        tfidf_documents.append(doc_tfidf)\n",
    "    return tfidf_documents\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(p*q for p,q in zip(vector1, vector2))\n",
    "    magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val**2 for val in vector2]))\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    return dot_product/magnitude\n",
    "\n",
    "tfidf_representation = tfidf(all_documents)\n",
    "our_tfidf_comparisons = []\n",
    "for count_0, doc_0 in enumerate(tfidf_representation):\n",
    "    for count_1, doc_1 in enumerate(tfidf_representation):\n",
    "        our_tfidf_comparisons.append((cosine_similarity(doc_0, doc_1), count_0, count_1))\n",
    "\n",
    "skl_tfidf_comparisons = []\n",
    "for count_0, doc_0 in enumerate(sklearn_representation.toarray()):\n",
    "    for count_1, doc_1 in enumerate(sklearn_representation.toarray()):\n",
    "        skl_tfidf_comparisons.append((cosine_similarity(doc_0, doc_1), count_0, count_1))\n",
    "\n",
    "for x in zip(sorted(our_tfidf_comparisons, reverse = True), sorted(skl_tfidf_comparisons, reverse = True)):\n",
    "    print(x)\n",
    "\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    print(len(intersection))\n",
    "    union = set(query).union(set(document))\n",
    "    print(len(union))\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "def jaccard_distance(a, b):\n",
    "    \"\"\"Calculate the jaccard distance between sets A and B\"\"\"\n",
    "    a = set(a)\n",
    "    b = set(b)\n",
    "    print(a, b)\n",
    "    return 1.0 * len(a&b)/len(a|b)\n",
    "\n",
    "all_diff = []\n",
    "all_lev = []\n",
    "all_sor = []\n",
    "for sent1, sent2 in list(zip(order_doc.sents, jane)):\n",
    "    t1 = sent1.text\n",
    "    t2 = sent2\n",
    "    diffl = difflib.SequenceMatcher(isjunk=None, a=t1, b=t2, autojunk=True).ratio()\n",
    "    lev = Levenshtein.ratio(t1, t2) \n",
    "    sor = 1 - distance.sorensen(t1, t2)\n",
    "    all_diff.append(diffl)\n",
    "    all_lev.append(lev)\n",
    "    all_sor.append(sor)\n",
    "    \n",
    "# diffl = difflib.SequenceMatcher(isjunk=None, a=order_doc.text, b=unorder_doc.text, autojunk=True).real_quick_ratio()\n",
    "seq = difflib.SequenceMatcher()\n",
    "seq.set_seqs(order_doc.text, jane)\n",
    "blocks = seq.get_matching_blocks()\n",
    "diffl = seq.ratio()\n",
    "med = statistics.median(all_sor)\n",
    "avg = statistics.mean(all_sor)\n",
    "pstd = statistics.pstdev(all_sor)\n",
    "sstd = statistics.stdev(all_sor)\n",
    "lev = Levenshtein.ratio(order_doc.text, jane) \n",
    "sor = 1 - distance.sorensen(order_doc.text, jane)\n",
    "\n",
    "print(diffl, lev, sor, med, avg, pstd, sstd)\n",
    "\n",
    "def cosine_similarity_ngrams(a, b, ngram):\n",
    "    counts_a = ngrams(a, ngram)\n",
    "    counts_b = ngrams(b, ngram)\n",
    "    vec1 = Counter(counts_a)\n",
    "    vec2 = Counter(counts_b)\n",
    "#     print('vec1', set(vec1.keys()), 'sum1', sum([vec1[x]**2 for x in vec1.keys()]), 'vec2', set(vec2.keys()), 'sum2', sum([vec2[x]**2 for x in vec2.keys()]))\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "#     print('intersection', numerator)\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "#     print('union', denominator)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    return float(numerator) / denominator\n",
    "print(\"Cosine: {}\".format(cosine_similarity_ngrams(order_list, unorder_list, 3)))\n",
    "print(\"Cosine: {}\".format(cosine_similarity_ngrams(order_doc_tokens, unorder_doc_tokens, 2)))\n",
    "\n",
    "print(len(ordered_text), len(unordered_text))\n",
    "diff_words = set(unordered_text).difference(ordered_text)\n",
    "print(len(diff_words))\n",
    "sys_diff = set(unordered_text).symmetric_difference(ordered_text)\n",
    "print(len(sys_diff))\n",
    "\n",
    "test = difflib.SequenceMatcher(None, unorder_text, order_text, autojunk=False).ratio()\n",
    "# https://stackoverflow.com/questions/4802137/how-to-use-sequencematcher-to-find-similarity-between-two-strings\n",
    "#difflib.SequenceMatcher uses the Ratcliff/Obershelp algorithm it computes the doubled number of matching characters divided by the total number of characters in the two strings.\n",
    "\n",
    "for t1, t2 in list(zip(order_tx, unorder_tx)):\n",
    "    diffl = difflib.SequenceMatcher(isjunk=None, a=t1, b=t2, autojunk=True).quickratio()\n",
    "    lev = Levenshtein.ratio(t1, t2) \n",
    "    sor = 1 - distance.sorensen(t1, t2)\n",
    "    all_metrics = [diffl, lev, sor]\n",
    "    med = statistics.median(all_metrics)\n",
    "    avg = statistics.mean(all_metrics)\n",
    "    pstd = statistics.pstdev(all_metrics)\n",
    "    sstd = statistics.stdev(all_metrics)\n",
    "    print('t1', t1, 't2', t2, all_metrics, med, avg, pstd, sstd)\n",
    "    \n",
    "def get_ngram(df1, df2):\n",
    "\n",
    "    df1_tokens = []\n",
    "    df1_text = []\n",
    "    for index, row in df1.iterrows():\n",
    "        raw_text = row['google_vision_text']\n",
    "        sents = nltk.sent_tokenize(raw_text)\n",
    "        for s in sents:\n",
    "            tokens = nltk.word_tokenize(s)\n",
    "            tokens = [t for t in tokens if t not in stopwords.words('english') and t not in string.punctuation]\n",
    "            text = ' '.join(tokens)\n",
    "            df1_tokens.append(tokens)\n",
    "            df1_text.append(text)\n",
    "        \n",
    "    df2_tokens = []\n",
    "    df2_text = []\n",
    "    for index, row in df2.iterrows():\n",
    "        raw_text = row['google_vision_text']\n",
    "        sents = nltk.sent_tokenize(raw_text)\n",
    "        for s in sents:\n",
    "            tokens = nltk.word_tokenize(s)\n",
    "            tokens = [t for t in tokens if t not in stopwords.words('english') and t not in string.punctuation]\n",
    "            text = ' '.join(tokens)\n",
    "            df2_tokens.append(tokens)\n",
    "            df2_text.append(text)\n",
    "    \n",
    "    return df1_tokens, df1_text, df2_tokens, df2_text\n",
    "order_tokens, order_tx, unorder_tokens, unorder_tx = get_ngram(eg_ordered, eg_unordered)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
