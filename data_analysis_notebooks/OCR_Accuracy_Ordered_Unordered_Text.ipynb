{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.models import ldamodel, doc2vec, LsiModel \n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import string\n",
    "import csv\n",
    "import math\n",
    "import statistics\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "# nltk.download('stopwords')\n",
    "from collections import OrderedDict, Counter, namedtuple\n",
    "import random\n",
    "import codecs, difflib, distance\n",
    "import rpy2\n",
    "from datasketch import MinHash\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R \n",
    "install.packages(\"textreuse\", repos='http://cran.us.r-project.org', quiet=TRUE)\n",
    "install.packages(\"readr\", repos='http://cran.us.r-project.org', quiet=TRUE)\n",
    "library(\"textreuse\")\n",
    "library(\"readr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First Test: Egyptian Gazette 1947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_unordered = pd.read_csv('ocr_test_newspaper_egyptian_gazette_one_page_unordered.csv')\n",
    "eg_ordered = pd.read_csv('ocr_test_newspaper_egyptian_gazette_one_page_ordered.csv')\n",
    "ocr_values = [eg_unordered['base_file_name'].iloc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Arab Scribe January 5 1964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_unordered = pd.read_csv('ocr_test_magazine_arab_scribe_unordered.csv')\n",
    "eg_ordered = pd.read_csv('ocr_test_magazine_arab_scribe_ordered.csv')\n",
    "\n",
    "eg_ordered['contains_image'].fillna(value=False, inplace=True)\n",
    "\n",
    "for index, row in eg_ordered.iterrows():\n",
    "    if math.isnan(row['page_number']):\n",
    "        pgn = row['base_file_name'].split('imagefile')[0][-3:]\n",
    "        pgn = pgn.split('_')[0]\n",
    "        eg_ordered.loc[index, 'page_number'] = int(pgn)\n",
    "\n",
    "groupby_df = eg_ordered.groupby('page_number')['google_vision_text'].apply(' '.join).reset_index()\n",
    "eg_ordered = eg_ordered.drop_duplicates(subset=['page_number'], keep='first')\n",
    "eg_ordered = eg_ordered.drop(columns='google_vision_text')\n",
    "final_df = pd.merge(eg_ordered, groupby_df, on='page_number', how='outer')\n",
    "eg_ordered = final_df.drop(columns='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words w/ or w/o punctuation and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenize(text):\n",
    "    if not text:\n",
    "#       print('The text to be tokenized is a None type. Defaulting to blank string.')\n",
    "        text = ''\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def process_text(df, punc):\n",
    "\n",
    "    final_doc = []\n",
    "    for index, row in df.iterrows():\n",
    "        raw_text = row['google_vision_text']\n",
    "        tokens = custom_tokenize(raw_text)\n",
    "        for t in tokens:\n",
    "            if punc:\n",
    "                if t in string.punctuation:\n",
    "                    pass\n",
    "                elif t in stopwords.words('english'):\n",
    "                    pass\n",
    "                else:\n",
    "                    final_doc.append(t.lower())\n",
    "            else: \n",
    "                final_doc.append(t.lower())\n",
    "    text = ' '.join(final_doc)\n",
    "    return final_doc, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_list, order_text = process_text(eg_ordered, True)\n",
    "unorder_list, unorder_text = process_text(eg_unordered, True)\n",
    "all_documents = [order_text, unorder_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Full Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_full_issue(all_documents, ocr_values, full_issues_ocr):\n",
    "    # Count n grams frequencies and calculate cosine similarity between two docs. \n",
    "    counts = CountVectorizer(ngram_range=(1,5))\n",
    "    counts_matrix = counts.fit_transform(all_documents)\n",
    "    cos = cosine_similarity(counts_matrix[0:1], counts_matrix)\n",
    "    print('Count Vectorizer', cos[0][1])\n",
    "    ocr_values.append(cos[0][1])\n",
    "    \n",
    "    # Calculate tf-idf cosine similarity (nltk or spacy text the same)\n",
    "    tokenize = lambda doc: doc.lower().split(\" \")\n",
    "    tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize, ngram_range=(1,5))\n",
    "    tfidf_matrix = tfidf.fit_transform(all_documents)\n",
    "\n",
    "    cos = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
    "    print('TF-IDF Vectorizer', cos)\n",
    "    ocr_values.append(cos[0][1])\n",
    "    \n",
    "    # Calculate similarity using GLOVE and SPACY\n",
    "    order_doc = nlp(order_text)\n",
    "    unorder_doc = nlp(unorder_text)\n",
    "    sim_doc = order_doc.similarity(unorder_doc)\n",
    "    print('Spacy GLOVE', sim_doc)\n",
    "    #https://stats.stackexchange.com/questions/304217/how-is-the-similarity-method-in-spacy-computed\n",
    "    ocr_values.append(sim_doc)\n",
    "    \n",
    "    # Calculate jaccard ratio. Takes list of tokens\n",
    "    jac = 1 - distance.jaccard(order_list, unorder_list)\n",
    "    print('Jaccard', jac)\n",
    "    ocr_values.append(jac)\n",
    "    \n",
    "    # use gensim's similarity matrix and lsi to calculate cosine\n",
    "    all_tokens = [order_list, unorder_list]\n",
    "    dictionary = Dictionary(all_tokens)\n",
    "    corpus = [dictionary.doc2bow(text) for text in all_tokens]\n",
    "    lsi = LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "    sim = MatrixSimilarity(lsi[corpus])\n",
    "    lsi_cos = [ t[1][1] for t in list(enumerate(sim))]\n",
    "    lsi_cos = lsi_cos[0]\n",
    "    print('LSI', lsi_cos)\n",
    "    ocr_values.append(lsi_cos)\n",
    "    #https://radimrehurek.com/gensim/tut3.html\n",
    "    \n",
    "    if os.path.isfile(full_issues_ocr):\n",
    "        final_metrics = pd.read_csv(full_issues_ocr)\n",
    "        ocr_values.append(datetime.date.today())\n",
    "        final_metrics.loc[len(final_metrics.index)] = ocr_values\n",
    "        final_metrics.to_csv(full_issues_ocr, index=False)\n",
    "    else:\n",
    "        cols = ['base_file_name', 'num_pages', 'countsvec_cos', 'tfidfvec_cos', 'spacy_sim', 'jaccard_sim', 'lsi_cos', 'date_run']\n",
    "        ocr_values.append(datetime.date.today())\n",
    "        final_df = pd.DataFrame([ocr_values], columns=cols)\n",
    "        final_df.to_csv(full_issues_ocr, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_values = [eg_unordered['base_file_name'].iloc[0], len(eg_unordered.index)]\n",
    "process_full_issue(all_documents, ocr_values, 'ocr_accuracy_full_issue_arab_scribe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Individual Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_pages(order, unorder):\n",
    "    %%R -i order\n",
    "    %%R -i unorder\n",
    "#     order <- read_file(\"order_doc.txt\")\n",
    "#     unorder <- read_file(\"unorder_doc.txt\")\n",
    "    %%R perfect = align_local(order, order)\n",
    "    %%R actual = align_local(order, unorder)\n",
    "    %%R smw <- actual$score / perfect$score\n",
    "    %%R smw\n",
    "    %%R -o smw\n",
    "    print(smw)\n",
    "    return smw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_page(all_documents, order_text, unorder_text, order_list, unorder_list, ocr_values, page_ocr):\n",
    "    # Count n grams frequencies and calculate cosine similarity between two docs. \n",
    "    counts = CountVectorizer(ngram_range=(1,5))\n",
    "    counts_matrix = counts.fit_transform(all_documents)\n",
    "    cos = cosine_similarity(counts_matrix[0:1], counts_matrix)\n",
    "    print('Count Vectorizer', cos[0][1])\n",
    "    ocr_values.append(cos[0][1])\n",
    "    \n",
    "    # Calculate tf-idf cosine similarity (nltk or spacy text the same)\n",
    "    tokenize = lambda doc: doc.lower().split(\" \")\n",
    "    tfidf = TfidfVectorizer(norm='l2',min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True, tokenizer=tokenize, ngram_range=(1,5))\n",
    "    tfidf_matrix = tfidf.fit_transform(all_documents)\n",
    "\n",
    "    cos = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
    "    print('TF-IDF Vectorizer', cos)\n",
    "    ocr_values.append(cos[0][1])\n",
    "    \n",
    "    # Calculate similarity using GLOVE and SPACY\n",
    "    order_doc = nlp(order_text)\n",
    "    unorder_doc = nlp(unorder_text)\n",
    "    sim_doc = order_doc.similarity(unorder_doc)\n",
    "    print('Spacy GLOVE', sim_doc)\n",
    "    #https://stats.stackexchange.com/questions/304217/how-is-the-similarity-method-in-spacy-computed\n",
    "    ocr_values.append(sim_doc)\n",
    "    \n",
    "    # Calculate jaccard ratio. Takes list of tokens\n",
    "    jac = 1 - distance.jaccard(order_list, unorder_list)\n",
    "    print('Jaccard', jac)\n",
    "    ocr_values.append(jac)\n",
    "    \n",
    "    # use gensim's similarity matrix and lsi to calculate cosine\n",
    "    all_tokens = [order_list, unorder_list]\n",
    "    dictionary = Dictionary(all_tokens)\n",
    "    corpus = [dictionary.doc2bow(text) for text in all_tokens]\n",
    "    lsi = LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "    sim = MatrixSimilarity(lsi[corpus])\n",
    "    lsi_cos = [ t[1][1] for t in list(enumerate(sim))]\n",
    "    lsi_cos = lsi_cos[0]\n",
    "    print('LSI', lsi_cos)\n",
    "    ocr_values.append(lsi_cos)\n",
    "    #https://radimrehurek.com/gensim/tut3.html\n",
    "    \n",
    "    align = align_pages(order_text, unorder_text)\n",
    "    print('smw', align)\n",
    "    ocr_values.append(align)\n",
    "    \n",
    "    if os.path.isfile(page_ocr):\n",
    "        final_metrics = pd.read_csv(page_ocr)\n",
    "        ocr_values.append(datetime.date.today())\n",
    "        final_metrics.loc[len(final_metrics.index)] = ocr_values\n",
    "        print(final_metrics)\n",
    "        final_metrics.to_csv(page_ocr, index=False)\n",
    "    else:\n",
    "        ocr_values.append(datetime.date.today())\n",
    "        cols = ['base_file_name', 'page_number', 'countsvec_cos', 'tfidfvec_cos', 'spacy_sim', 'jaccard_sim', 'lsi_cos','smw_align', 'date_run']\n",
    "        final_df = pd.DataFrame([ocr_values], columns=cols)\n",
    "        final_df.to_csv(page_ocr, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(eg_unordered.index)):\n",
    "    ocr_values = [eg_unordered['base_file_name'].iloc[i], i]\n",
    "    order_list, order_text = process_text(eg_ordered.iloc[[i]], True)\n",
    "    unorder_list, unorder_text = process_text(eg_unordered.iloc[[i]], True)\n",
    "    all_documents = [order_text, unorder_text]\n",
    "    process_page(all_documents, order_text, unorder_text, order_list, unorder_list, ocr_values, 'ocr_accuracy_page_level_arab_scribe.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
